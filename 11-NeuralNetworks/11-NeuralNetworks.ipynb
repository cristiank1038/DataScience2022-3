{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8fa137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br/>\n",
    "<img src=\"images/cd-logo-blue-600x600.png\" alt=\"\" width=\"130px\" align=\"left\"/>\n",
    "<img src=\"images/cd-logo-blue-600x600.png\" alt=\"\" width=\"130px\" align=\"right\"/>\n",
    "<div align=\"center\">\n",
    "<h2>Bootcamp Data Science - Módulo 3</h2><br/>\n",
    "<h1>Introducción a Redes Neuronales y DeepLearning</h1>\n",
    "<br/><br/>\n",
    "    <b>Instructor Principal:</b> Patricio Olivares polivares@codingdojo.cl <br/>\n",
    "    <b>Instructor Asistente:</b> Jesús Ortiz jortiz@codingdojo.cl<br/><br/>\n",
    "    <b>Coding Dojo</b>\n",
    "</div>\n",
    "<br>\n",
    "Fuente: \"Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d349ab6",
   "metadata": {},
   "source": [
    "# Redes neuronales (Neural Networks - NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a7e4c",
   "metadata": {},
   "source": [
    "- Las Redes Neuronales son un conjunto de modelos de Machine Learning.\n",
    "- Estas se inspiran en el funcionamiento neurológico del cerebro humano.\n",
    "- Las redes neuronales han sido utilizadas en múltiples y diferentes áreas de aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ee6d1",
   "metadata": {},
   "source": [
    "### Ejemplos de aplicaciones Deep Learning\n",
    "\n",
    "- [DeepMind](https://www.youtube.com/watch?v=V1eYniJ0Rnk)\n",
    "- [Gaugan2](http://gaugan.org/gaugan2/?text_input=ocean+waves+in+a+beach)\n",
    "- [YoloV3](https://www.youtube.com/watch?v=MPU2HistivI)\n",
    "- [QuickDraw](https://quickdraw.withgoogle.com/#)\n",
    "- [Rock-Paper-Scisors](https://tenso.rs/demos/rock-paper-scissors/)\n",
    "- Y una laaaarga [lista](https://www.mygreatlearning.com/blog/deep-learning-applications/#deepdreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee2e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78436bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es la red neuronal más sencilla cuya base es la TLU (Threshold Logic Unit)\n",
    "- TLU es una neurona artificial que opera de la siguiente manera\n",
    "    - Recibe una o varias entradas numéricas $(x_1, x_2, ..., x_n)$\n",
    "    - Cada entrada es ponderada por un peso $(w_1x_1, w_2x_2, ..., w_nx_n)$\n",
    "    - Se calcula la suma ponderada de las entradas $z=w_1x_1 + w_2x_2 + ... +w_nx_n $\n",
    "    - Al resultado de la suma ponderada se le aplica una función escalonada $h_w(x)=\\text{step}(z)$. Esta es nuestra predicción\n",
    "- Al entrenar una neurona, lo que se intenta determinar son los **pesos** $w$ tales que la predicción $h_w(x)$ sea lo más cercana al valor real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a6de1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/perceptron.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# iris = load_iris()\n",
    "# X = iris.data[:,(2,3)] # Largo y ancho del pétalo\n",
    "# y = (iris.target == 0) # Iris setosa?\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "per = Perceptron()\n",
    "per.fit(X_train, y_train)\n",
    "print(\"Training score:\", per.score(X_train, y_train))\n",
    "print(\"Test score:\", per.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb51c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes multicapas y Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651bcce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es posible conectar múltiples neuronas entre sí tomando la salida de una neurona como la entrada para una o múltiples neuronas adicionales.\n",
    "- Las conexiones que se producen entre neuronas conforman lo que se llama como **red neuronal**.\n",
    "- Dependiendo de cuales sean las conexiones entre distintas neuronas, se habla de la **arquitectura** de la red neuronal.\n",
    "- Las redes neuronales se dividen en **capas**, las cuales se dividen en las siguientes categorías:\n",
    "    - Capas de entrada (input layers)\n",
    "    - Capas de salida (output layers)\n",
    "    - Capas ocultas/intermedias (hidden layers)\n",
    "- Las redes multicapas están compuestas por una capa de entrada, una o varias capas ocultas y una capa de salida\n",
    "- Cuando una red multicapa contiene muchas capas ocultas, hablamos de una **Red Neuronal Profunda** (Deep Neural Network - DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b1843",
   "metadata": {},
   "source": [
    "<img src=\"images/dnn.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8178fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que en el perceptron, deseamos calcular los pesos $w$ de todas las neuronas tales que la salida esté lo más cercana a los valores esperados. \n",
    "- ¿Cómo podemos calcular tantos pesos a la vez? \n",
    "    - **R:** Backpropagation (para quienes estén interesados en estudiar más a fondo, pueden buscar el algoritmo de Descenso del Gradiente/Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468f774",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bibliotecas de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0334dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Actualmente existe una amplia gama de bibliotecas para Deep Learning. \n",
    "- Por su facilidad de uso y popularidad, utilizaremos la API de **Keras**, la cual viene incluida dentro de la biblioteca **Tensorflow**.\n",
    "- Para poder utilizar Keras, es necesario tener previamente instalado Tensorflow (```pip install tensorflow```)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61a9ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/bibliotecas.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdde7a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creando redes neuronales en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1b71c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para crear una red neuronal, debemos\n",
    "- Definir la arquitectura de la red (modelo y capas)\n",
    "- Compilar el modelo (selección de optimizados, función de pérdida y métricas)\n",
    "- Entrenar modelo\n",
    "- Usar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c60ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo usando fashion mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Cost\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"] # 10 clases\n",
    "\n",
    "X_train_full = X_train_full/255.0 # Escalamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b68ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(6,6,i+1)\n",
    "    ax.imshow(X_train_full[i], cmap='gist_yarg')\n",
    "    ax.set_title(class_names[y_train_full[i]])\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado de nuestra red neuronal\n",
    "model = tf.keras.models.Sequential() # Modelo secuencial\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28])) # Capa de entrada, tiene dimensiones 28x28\n",
    "model.add(tf.keras.layers.Dense(300, activation='relu')) # Primera capa oculta, salida 300\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu')) # Segunda capa oculta, salida 100\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax')) # Capa de salida, salida 10 (número de clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red creada\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9366c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilización de datos de validación y entrenamiento del modelo\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, \n",
    "                                                  stratify=y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=10000,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fe7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/255.0 # nuevamente, escalamiento de set de datos de entrenamiento\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=32)\n",
    "print(\"Score:\", score) # Función de pérdida\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "dato = 56\n",
    "# dato=12\n",
    "prediction = model.predict(X_test)\n",
    "classes_x=np.argmax(prediction,axis=1)\n",
    "print(\"Predicción:\", class_names[classes_x[dato]])\n",
    "print(\"Clase Real:\", class_names[y_test[dato]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cd8eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularización en NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56121e67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- La gran cantidad de parámetros que posee una red neuronal, si bien, entrega una gran flexbilidad de ajuste a una enorme cantidad de datos distintos, esto también las hace propensas al overfitting.\n",
    "- Para evitar el overfitting, al igual en otros métodos, necesitamos utilizar técnicas de **regularización**.\n",
    "- Algunas de las técnicas más populares son:\n",
    "    - Regularización $l_1$ y $l_2$\n",
    "    - Dropout (dilución)\n",
    "    - Early stop (detención anticipada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nba.csv', index_col = 'Name')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botar faltantes\n",
    "df.dropna(inplace = True)\n",
    "# Guardar X datos\n",
    "X = df.drop(columns = 'TARGET_5Yrs')\n",
    "# Codificar nuestro objetivo\n",
    "y = df['TARGET_5Yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División entrenamiento, validación, prueba\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a504ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamiento\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Input shape\n",
    "input_shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752da56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo sin regularización\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19,\n",
    "                input_dim = input_shape, \n",
    "                activation = 'relu')) \n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy']) # bce: binary cross entropy\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07410f2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar la perdida\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd5936",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularización $l_1$ y $l_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581183c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que lo visto en las regresiones de Ridge/Lasso, las regularización $l_1$ y $l_2$ agregan factores a la función de costo de una red neuronal para impedir que los pesos de esta tomen cualquier valor.\n",
    "- Para agregar regularización a una capa, se debe utilizar lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ecb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "# Con Regularización l2\n",
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19, # Cuántas neuronas tienes en tu primera capa oculta\n",
    "                input_dim = input_shape, # ¿Cuál es la forma de tus características de entrada (número de columnas)?\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer=l2(0.01))) # ¿Qué función de activación estás usando?\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu',\n",
    "                kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy'])\n",
    "history_l2 = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_l2.history['loss'], label='Train loss')\n",
    "plt.plot(history_l2.history['val_loss'], label='Val Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eabdd",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e087b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Un algoritmo sencillo que ha mostrado ser muy efectivo al entrenar redes neuronales\n",
    "- En cada iteración de entrenamiento, cada neurona (excepto las neuronas de salida) tiene una probabilidad $p$ de ser **excluída** (ignorada).\n",
    "- La probabilidad $p$ es llamada **tasa de dropout**. Por lo general esta tasa se escoge como:\n",
    "    - Entre 40%-50% para redes convolucionales\n",
    "    - Entre 20%-30% para redes recurrentes\n",
    "    - Para otras redes, se puede escoger valores entre el 10% hasta el 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c26766",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Con Dropout\n",
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19, # Cuántas neuronas tienes en tu primera capa oculta\n",
    "                input_dim = input_shape, # ¿Cuál es la forma de tus características de entrada (número de columnas)?\n",
    "                activation = 'relu')) # ¿Qué función de activación estás usando?\n",
    "# A continuación agregamos nuestra capa de dropout\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy'])\n",
    "history_dropout = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daab4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_dropout.history['loss'], label='Train loss')\n",
    "plt.plot(history_dropout.history['val_loss'], label='Val Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cee69",
   "metadata": {},
   "source": [
    "# Early stop (detención temprana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f9322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Para algoritmos iterativos que tienden al overfitting, el early stopping propone dejar de entrenar tan pronto se alcanza el mínimo error. \n",
    "- En el ejemplo se ve que el error de validación aumenta en la medida que aumentan los epochs. Early stop mantendrá aquel modelo que entrega el menor error de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9336cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Con detención anticipada\n",
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19, # Cuántas neuronas tienes en tu primera capa oculta\n",
    "                input_dim = input_shape, # ¿Cuál es la forma de tus características de entrada (número de columnas)?\n",
    "                activation = 'relu')) # ¿Qué función de activación estás usando?\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience = 5) # Número de iteraciones sin mejora antes de parar el entrenamiento\n",
    "history_earlystop = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_test, y_test), \n",
    "                    epochs=100,\n",
    "                    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_earlystop.history['loss'], label='Train loss')\n",
    "plt.plot(history_earlystop.history['val_loss'], label='Validation Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe47f38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Redes Neuronales Recurrentes (RNN-Recurrent Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200b4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hasta el momento hemos trabajado con redes FeedForward, donde las activaciones fluyen en una dirección: desde la capa de entrada a la capa de salida.\n",
    "- Las redes neuronales recurrentes son similares a las redes FeedForward, salvo que también existen conexiones en el sentido inverso.\n",
    "- Las redes neuronales recurrentes son utilizadas generalmente para procesar secuencias de datos temporalmente relacionados, pues toman en cuenta lo que ocurrió previamente\n",
    "- Algunas arquitecturas de redes recurrentes:\n",
    "    - Long Short Term Memory (LSTM)\n",
    "    - Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8170716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fc1af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset de manchas solares disponible en https://github.com/jbrownlee/Datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "df = pd.read_csv('data/sunspots.csv', \n",
    "                 index_col = 'Date')\n",
    "df = df.drop(columns=['id'])\n",
    "df.rename(columns={'Monthly Mean Total Sunspot Number': 'sunspots'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3765cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "df['sunspots'].plot()\n",
    "plt.ylabel('Manchas solares');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6494285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test\n",
    "train_values = np.array(df.loc[:'2016-12-31', 'sunspots'])\n",
    "test_values = np.array(df.loc['2017-01-31':, 'sunspots'])\n",
    "\n",
    "# Escalamiento\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "train = scaler.fit_transform(train_values.reshape(-1, 1))\n",
    "test = scaler.transform(test_values.reshape(-1, 1))\n",
    "\n",
    "# Conversión a datos con tres columnas\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "prev_months = 2\n",
    "generator = timeseries_dataset_from_array(\n",
    "    train,\n",
    "    train,\n",
    "    sequence_length=prev_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab68428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Capa LSTM\n",
    "model.add(LSTM(units = 50, \n",
    "               return_sequences = False # True si la siguiente capa será una capa recurrente\n",
    "               ))\n",
    "# Capa de salida\n",
    "model.add(Dense(units = 1, activation = 'relu'))\n",
    "\n",
    "# Compilado\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "history_lstm = model.fit(generator, epochs=200) # el generador creará los datos en cada iteración\n",
    "\n",
    "plt.plot(history_lstm.history['loss'], label='Train loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "full = np.concatenate((train, test))\n",
    "test_days = pd.DataFrame(full)[len(full) - len(test) - prev_months:].values\n",
    "generator_test = timeseries_dataset_from_array(\n",
    "    test_days,\n",
    "    test_days,\n",
    "    sequence_length=prev_months)\n",
    "\n",
    "# Predicciones\n",
    "preds = model.predict(generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "plt.plot(scaler.inverse_transform(test_days), label='Actual sunspots')\n",
    "plt.plot(scaler.inverse_transform(preds), label='Predicted sunspots')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca9cc2",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales (CNN-Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50d55d",
   "metadata": {},
   "source": [
    "- Así como las redes neuronales recurrentes permiten procesar datos temporalmente relacionados, las redes neuronales convolucionales permiten procesar datos espacialmente relacionados.\n",
    "- Este tipo de redes son utilizadas generalmente para procesar imágenes, donde los pixeles están relacionados espacialmente entre sí.\n",
    "- En las redes convolucionales, generalmente se utilizan **capas de pooling**. Estas capas se utilizan para reducción de dimensionalidad, disminuyendo la redundancia de datos existente en imágenes (pixeles cercanos generalmente están altamente correlacionados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ea104",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo, fuente https://www.tensorflow.org/tutorials/images/cnn\n",
    "# Bibliotecas\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e75265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de modelo\n",
    "model = models.Sequential()\n",
    "# Capas encargadas de obtener información de la imagen\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# Capas para la clasificación en base a la información obtenida en \n",
    "# capas anteriores\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be900643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_cnn = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452581c",
   "metadata": {},
   "source": [
    "# Actividad 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f258c",
   "metadata": {},
   "source": [
    "- Descargue y analice el dataset presente en el siguiente enlace de Kaggle: https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data\n",
    "- Cree un modelo capaz de reconocer si una foto corresponde a la foto de un gato o un perro. Para ello, compare distintos modelos y quédese con aquel con mejor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a3261",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "#codigo extra, para que imagenes de matplotlib\n",
    "#estén centradas en las diapositivas, ejecutar antes de lanzar los ejemplos."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
